{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment BDM\n",
    "\n",
    "# ISB ID: 71710035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.HiveContext at 0x7f239942fdd0>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_users = sc.textFile('file:///home/cloudera/Desktop/Assignment/users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_transactions = sc.textFile('file:///home/cloudera/Desktop/Assignment/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user = rdd_users.map(lambda x: x.split(\",\")).toDF(['Userid', 'Emailid',\\\n",
    "                                                   'NativeLanguage','Location'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction = rdd_transactions.map(lambda x: x.split(\",\")).toDF(['TransactionID',\\\n",
    "                                                                 'ProductID'\\\n",
    "                                                                ,'Userid', 'Price', \\\n",
    "                                                                 'Productdecription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\tCount of unique locations where each product is sold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Userid=u'1', Emailid=u'user1@company.com', NativeLanguage=u'ES', Location=u'MX', TransactionID=u'11', ProductID=u'1004', Price=u'129', Productdecription=u'whatchamacallit'),\n",
       " Row(Userid=u'2', Emailid=u'user4@domain.com', NativeLanguage=u'EN', Location=u'US', TransactionID=u'12', ProductID=u'1004', Price=u'129', Productdecription=u'whatchamacallit'),\n",
       " Row(Userid=u'3', Emailid=u'user5@company.com', NativeLanguage=u'FR', Location=u'FR', TransactionID=u'5', ProductID=u'1003', Price=u'89', Productdecription=u'gadget'),\n",
       " Row(Userid=u'5', Emailid=u'user12@service.io', NativeLanguage=u'EN', Location=u'CA', TransactionID=u'13', ProductID=u'1005', Price=u'199', Productdecription=u'doohickey'),\n",
       " Row(Userid=u'6', Emailid=u'user17@website.net', NativeLanguage=u'FR', Location=u'FR', TransactionID=u'10', ProductID=u'1003', Price=u'89', Productdecription=u'gadget'),\n",
       " Row(Userid=u'7', Emailid=u'user21@company.com', NativeLanguage=u'FR', Location=u'FR', TransactionID=u'14', ProductID=u'1004', Price=u'129', Productdecription=u'whatchamacallit'),\n",
       " Row(Userid=u'9', Emailid=u'user27@school.edu', NativeLanguage=u'ES', Location=u'MX', TransactionID=u'4', ProductID=u'1001', Price=u'99', Productdecription=u'thingamajig'),\n",
       " Row(Userid=u'10', Emailid=u'user31@website.net', NativeLanguage=u'EN', Location=u'CA', TransactionID=u'2', ProductID=u'1001', Price=u'99', Productdecription=u'thingamajig'),\n",
       " Row(Userid=u'16', Emailid=u'user53@school.edu', NativeLanguage=u'EN', Location=u'US', TransactionID=u'15', ProductID=u'1002', Price=u'149', Productdecription=u'gizmo'),\n",
       " Row(Userid=u'17', Emailid=u'user57@school.edu', NativeLanguage=u'ES', Location=u'MX', TransactionID=u'3', ProductID=u'1004', Price=u'129', Productdecription=u'whatchamacallit'),\n",
       " Row(Userid=u'19', Emailid=u'user64@school.edu', NativeLanguage=u'EN', Location=u'US', TransactionID=u'1', ProductID=u'1004', Price=u'129', Productdecription=u'whatchamacallit'),\n",
       " Row(Userid=u'19', Emailid=u'user64@school.edu', NativeLanguage=u'EN', Location=u'US', TransactionID=u'6', ProductID=u'1002', Price=u'149', Productdecription=u'gizmo'),\n",
       " Row(Userid=u'22', Emailid=u'user71@domain.com', NativeLanguage=u'ES', Location=u'MX', TransactionID=u'9', ProductID=u'1001', Price=u'99', Productdecription=u'thingamajig'),\n",
       " Row(Userid=u'26', Emailid=u'user85@service.io', NativeLanguage=u'HI', Location=u'IN', TransactionID=u'8', ProductID=u'1002', Price=u'149', Productdecription=u'gizmo'),\n",
       " Row(Userid=u'30', Emailid=u'user99@website.net', NativeLanguage=u'EN', Location=u'US', TransactionID=u'7', ProductID=u'1002', Price=u'149', Productdecription=u'gizmo')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Inner Join on two dataframe of users and transaction\n",
    "loc = user.join(transaction, [\"Userid\"], 'inner')\n",
    "loc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Location|count|\n",
      "+--------+-----+\n",
      "|      MX|    4|\n",
      "|      US|    5|\n",
      "|      IN|    1|\n",
      "|      CA|    2|\n",
      "|      FR|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count by Location\n",
    "count_loc = loc.groupBy('Location').count()\n",
    "count_loc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since there is some confusion regarding grouping by product or location.  \n",
    "#This is an attempt to group by product\n",
    "product = loc.map(lambda y: ((int(y[5])), (y[3]))).toDF(['ProductID','Location']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|ProductID|count|\n",
      "+---------+-----+\n",
      "|     1001|    2|\n",
      "|     1002|    2|\n",
      "|     1003|    1|\n",
      "|     1004|    3|\n",
      "|     1005|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouping= product.groupBy(\"ProductID\")\n",
    "grouping.count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tFind out products bought by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|Userid|Productdecription|\n",
      "+------+-----------------+\n",
      "|     1|  whatchamacallit|\n",
      "|     2|  whatchamacallit|\n",
      "|     3|           gadget|\n",
      "|     5|        doohickey|\n",
      "|     6|           gadget|\n",
      "|     7|  whatchamacallit|\n",
      "|     9|      thingamajig|\n",
      "|    10|      thingamajig|\n",
      "|    16|            gizmo|\n",
      "|    17|  whatchamacallit|\n",
      "|    19|  whatchamacallit|\n",
      "|    19|            gizmo|\n",
      "|    22|      thingamajig|\n",
      "|    26|            gizmo|\n",
      "|    30|            gizmo|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b) Find out products bought by each user.\n",
    "loc.select('Userid', 'Productdecription').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\tTotal spending done by each user on each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price_change= loc.map(lambda y: ((int(y[0])), float(y[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 129.0), (2, 129.0), (3, 89.0), (5, 199.0), (6, 89.0), (7, 129.0), (9, 99.0), (10, 99.0), (16, 149.0), (17, 129.0), (19, 129.0), (19, 149.0), (22, 99.0), (26, 149.0), (30, 149.0)]\n"
     ]
    }
   ],
   "source": [
    "print (price_change.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c)\tTotal spending done by each user on each product.\n",
    "\n",
    "sum_price = price_change.reduceByKey(lambda x,y: x+y)\n",
    "type(sum_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|userid|TotalPrice|\n",
      "+------+----------+\n",
      "|     1|     129.0|\n",
      "|     2|     129.0|\n",
      "|     3|      89.0|\n",
      "|     5|     199.0|\n",
      "|     6|      89.0|\n",
      "|     7|     129.0|\n",
      "|     9|      99.0|\n",
      "|    10|      99.0|\n",
      "|    16|     149.0|\n",
      "|    17|     129.0|\n",
      "|    19|     278.0|\n",
      "|    22|      99.0|\n",
      "|    26|     149.0|\n",
      "|    30|     149.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_price.toDF(['userid', 'TotalPrice']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----------------+--------------------+-------------------+\n",
      "|             country|                id|            place|                text|               user|\n",
      "+--------------------+------------------+-----------------+--------------------+-------------------+\n",
      "|               India|572692378957430785|           Orissa|@always_nidhi @Yo...|    Srkian_nishu :)|\n",
      "|       United States|572575240615796737|        Manhattan|@OnlyDancers Bell...| TagineDiningGlobal|\n",
      "|       United States|572575243883036672|        Claremont|1/ \"Without the a...|        Daniel Beer|\n",
      "|       United States|572575252020109313|           Vienna|idk why people ha...|   someone actually|\n",
      "|       United States|572575274539356160|           Boston|Taste of Iceland!...|     BostonAttitude|\n",
      "|       United States|572647819401670656|          Suwanee|Know what you don...|Collin A. Zimmerman|\n",
      "|           Indonesia|572647831053312000|      Mario Riawa|Serasi ade haha @...|   Rinie Syamsuddin|\n",
      "|           Indonesia|572647839521767425|    Bogor Selatan|Akhirnya bisa jug...|       Vinny Sylvia|\n",
      "|       United States|572647841220337664|          Norwalk|@BeezyDH_ it's li...|                Cas|\n",
      "|       United States|572647842277396480|           Santee| obsessed with music|               kimo|\n",
      "|       United States|572631750163234816|        Tennessee|@blakeshelton You...|        Jeff Morton|\n",
      "|           Indonesia|572631763115249664|           Gambir|Happy Birhday Ps....|        Rensus Paul|\n",
      "|       United States|572606799712428033|   North Carolina|One night I'm ext...|                 KC|\n",
      "|       United States|572606799649640449|        Baltimore|@DjGregStreet STO...|      #QuissyUpSoon|\n",
      "|       United States|572606809216663552|          Cypress|always getting in...|                 lo|\n",
      "|Negara Brunei Dar...|572606812081410048|           Brunei|nigga in paris ht...|           hafizzul|\n",
      "|       United States|572616136963055616|         Portland|Boutta fall aslee...|          Princess✨|\n",
      "|       United States|572616139987144704|         Kentucky|Canadians are tak...|       Nene Kiameso|\n",
      "|       United States|572616165786185728|         Wahpeton|Chicago takin ove...|              Chase|\n",
      "|      United Kingdom|572667477949353984|Ashton-under-Lyne|@traceyb65 I'm up...|             stefan|\n",
      "+--------------------+------------------+-----------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.jsonFile(\"file:///home/cloudera/Desktop/Assignment/tweets.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Schema for the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- place: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[country: string, id: string, place: string, text: string, user: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caching the data form\n",
    "df.registerTempTable(\"df\")\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all of the tweets made by a user (any user would work. We should be able to replace user names to get tweets by that particular user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the user name:stefan\n",
      "stefan\n",
      "+--------------------+-------+\n",
      "|                text|handler|\n",
      "+--------------------+-------+\n",
      "|@traceyb65 I'm up...| stefan|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_user = raw_input(\"Please enter the user name:\")\n",
    "print input_user\n",
    "sqlContext.sql(\"SELECT text, user as handler FROM df WHERE user LIKE '{}'\".format(input_user)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find count of all tweets by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|                user|NumberofTweets|\n",
      "+--------------------+--------------+\n",
      "|       #QuissyUpSoon|           258|\n",
      "|Inès Mendes Askiip ♥|           185|\n",
      "|           #4Rentinc|           100|\n",
      "|                  MV|            58|\n",
      "|    williampriceking|            46|\n",
      "+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#c)\tFind count of all tweets by each user.\n",
    "\n",
    "sqlContext.sql(\"SELECT user, count(1) as NumberofTweets FROM df GROUP BY \\\n",
    "user ORDER BY NumberofTweets DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all of the people who are mentioned in tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|        person|\n",
      "+--------------+\n",
      "|  always_nidhi|\n",
      "|   OnlyDancers|\n",
      "|IcelandNatural|\n",
      "|       AdeRais|\n",
      "|      BeezyDH_|\n",
      "|  blakeshelton|\n",
      "|  DjGregStreet|\n",
      "|     traceyb65|\n",
      "|     hokkazonn|\n",
      "|       daunugh|\n",
      "|        3Will1|\n",
      "| 4fucksakesmag|\n",
      "|  XAmbassadors|\n",
      "|    Ashton5SOS|\n",
      "|  jamiisonkent|\n",
      "|    Parisblass|\n",
      "|     __akgrown|\n",
      "|              |\n",
      "|     Jazzfeezy|\n",
      "|   NBCTheVoice|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#d)\tGet a list of all of the people who are mentioned in tweets.\n",
    "sqlContext.sql(\"SELECT regexp_extract(text,'@([A-Za-z0-9_]+)') as person FROM df \\\n",
    "WHERE text LIKE '%@%'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of time each person is mentioned in the entire dataset of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|person_intweets|count|\n",
      "+---------------+-----+\n",
      "|servaescolton16|    1|\n",
      "|     NICKIMINAJ|    1|\n",
      "| mitasukamdiyah|    1|\n",
      "|   ThatsSarcasm|    1|\n",
      "|ComedyWorIdStar|    1|\n",
      "|      lecamping|    1|\n",
      "|         MAEJOR|    1|\n",
      "|  Lange_Records|    1|\n",
      "|        DJSWHiT|    1|\n",
      "|       sadables|    1|\n",
      "| itstonybennett|    1|\n",
      "|    ParisHilton|    1|\n",
      "|     em__singer|    1|\n",
      "|     JayRansiky|    1|\n",
      "|        brikeey|    1|\n",
      "|    _CharquellC|    1|\n",
      "|          jhene|    1|\n",
      "|    ShawnMendes|    1|\n",
      "|      VAME_2015|    1|\n",
      "|   NiykeeHeaton|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT regexp_extract(text,'@([A-Za-z0-9_]+)') as person_intweets, count(*)\\\n",
    " as count FROM df WHERE text like '%@%' GROUP BY text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give top 50 users who are mentioned the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                user|Top50|\n",
      "+--------------------+-----+\n",
      "|       #QuissyUpSoon|  258|\n",
      "|Inès Mendes Askiip ♥|  185|\n",
      "|           #4Rentinc|  100|\n",
      "|                  MV|   58|\n",
      "|    williampriceking|   46|\n",
      "|✌ Follow Me MAEJOR ✌|   44|\n",
      "|    Phillthy McNasty|   43|\n",
      "|       K.O.H.O.R.T.S|   41|\n",
      "|  #AMNT KINGTAECRAZY|   41|\n",
      "|        Ghafla.co.ke|   36|\n",
      "|        Ully U Music|   35|\n",
      "|            Codeclic|   33|\n",
      "|           Lord Dash|   30|\n",
      "|  TagineDiningGlobal|   30|\n",
      "|      Herri Setiawan|   29|\n",
      "|          Dell Feddi|   29|\n",
      "|   Kidrauhl Forever❤|   25|\n",
      "|     Trendsmap Paris|   23|\n",
      "|      #TurnYaSneakUp|   22|\n",
      "|                Bel |   19|\n",
      "|   Change Barcelona!|   18|\n",
      "|          alec reyes|   17|\n",
      "|          Lil Shadow|   17|\n",
      "|           arianator|   15|\n",
      "|Music Dir. F.Herrera|   14|\n",
      "|       davidfelician|   13|\n",
      "|  Terra Rising Films|   12|\n",
      "|ST★RS RADIO 91,6 MHz|   12|\n",
      "|         TheBeachKid|   12|\n",
      "|   Ecotour.com Deals|   11|\n",
      "|        Lionel Rigal|   11|\n",
      "|     Naijaloaded.com|   10|\n",
      "| @TranceAndGoodvibes|   10|\n",
      "|      djmacdaddyshow|   10|\n",
      "|          Bobby Borg|    9|\n",
      "|     Levanta Brasil |    9|\n",
      "|       Dana Penland |    9|\n",
      "|     Virtual Jukebox|    9|\n",
      "|   ARS RADIO STATION|    8|\n",
      "|30A Songwriter Radio|    8|\n",
      "|    Michael S. Osman|    8|\n",
      "|  DR. Kern M Quaccoo|    8|\n",
      "|                   .|    7|\n",
      "|            EnigmaCG|    7|\n",
      "|          Langelihle|    7|\n",
      "|              luizko|    7|\n",
      "|                 Ana|    7|\n",
      "|     thedreamchazer™|    7|\n",
      "|        BuchananBoyz|    6|\n",
      "|          Nidal Eses|    6|\n",
      "+--------------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#f)\tGive top 50 users who are mentioned the most.\n",
    "\n",
    "sqlContext.sql(\"SELECT user, count(*) as Top50 FROM df GROUP BY user ORDER BY Top50 DESC\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         person|count|\n",
      "+---------------+-----+\n",
      "|     NICKIMINAJ|    2|\n",
      "|       MeekMill|    2|\n",
      "|            GMA|    2|\n",
      "|     1goodtexan|    2|\n",
      "|servaescolton16|    1|\n",
      "|     NICKIMINAJ|    1|\n",
      "| mitasukamdiyah|    1|\n",
      "|   ThatsSarcasm|    1|\n",
      "|ComedyWorIdStar|    1|\n",
      "|      lecamping|    1|\n",
      "|         MAEJOR|    1|\n",
      "|  Lange_Records|    1|\n",
      "|        DJSWHiT|    1|\n",
      "|       sadables|    1|\n",
      "| itstonybennett|    1|\n",
      "|    ParisHilton|    1|\n",
      "|     em__singer|    1|\n",
      "|     JayRansiky|    1|\n",
      "|        brikeey|    1|\n",
      "|    _CharquellC|    1|\n",
      "|          jhene|    1|\n",
      "|    ShawnMendes|    1|\n",
      "|      VAME_2015|    1|\n",
      "|   NiykeeHeaton|    1|\n",
      "| TheeDeanAugust|    1|\n",
      "|     AshleyJudd|    1|\n",
      "|               |    1|\n",
      "|      MszDeevah|    1|\n",
      "|          kfury|    1|\n",
      "|  ChrisMastamp3|    1|\n",
      "|   ComptonMovie|    1|\n",
      "|      infoyogya|    1|\n",
      "|          train|    1|\n",
      "|     AsifAAhmad|    1|\n",
      "|       WatchGHS|    1|\n",
      "|  DisneylandDTD|    1|\n",
      "|    jesusss_123|    1|\n",
      "|NYCFoodieFinder|    1|\n",
      "|        Jim_B60|    1|\n",
      "|   taylorcaniff|    1|\n",
      "|       imbadder|    1|\n",
      "|info_purwakarta|    1|\n",
      "|  thedoobiebros|    1|\n",
      "|     riandawson|    1|\n",
      "|       Juppel55|    1|\n",
      "|MadisonElleBeer|    1|\n",
      "|      WhoDat_4D|    1|\n",
      "|    lunalindsey|    1|\n",
      "|   MissyElliott|    1|\n",
      "|        hi_mija|    1|\n",
      "+---------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 50 person mentioned in tweets\n",
    "sqlContext.sql(\"SELECT regexp_extract(text,'@([A-Za-z0-9_]+)') as person, count(*) as count FROM df  WHERE \\\n",
    "text like '%@%' GROUP BY text ORDER BY count(*) DESC\").distinct().show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all hashtags mentioned in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|        person|\n",
      "+--------------+\n",
      "|shutUpAndDANCE|\n",
      "|        SHINee|\n",
      "|          JR50|\n",
      "|           DME|\n",
      "|    nowplaying|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#g)\tGet a list of all hashtags mentioned in the dataset.\n",
    "\n",
    "hashtag = sqlContext.sql(\"SELECT regexp_extract(text,'#([A-Za-z0-9_]+)') as \\\n",
    "person FROM df WHERE text LIKE '%#%'\")\n",
    "hashtag.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find how many times each hashtag is mentioned in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            handlers|count|\n",
      "+--------------------+-----+\n",
      "|        RumiPlaylist|    1|\n",
      "|                  NO|    1|\n",
      "|             Detroit|    1|\n",
      "|                 art|    1|\n",
      "|               REACH|    1|\n",
      "|    papoDeAtleticano|    1|\n",
      "|        bdscertified|    1|\n",
      "|      DetroitStandUp|    1|\n",
      "|TourAlbumBergerak...|    1|\n",
      "|           education|    1|\n",
      "|              Studio|    1|\n",
      "|            fainting|    1|\n",
      "|          FlyAmerica|    1|\n",
      "|            lemonsup|    1|\n",
      "|          NowPlaying|    1|\n",
      "|             blessed|    1|\n",
      "|                Tour|    1|\n",
      "|                NeYo|    1|\n",
      "|         SabineWeiss|    1|\n",
      "|            podscast|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#h)\tFind how many times each hashtag is mentioned in the dataset.\n",
    "\n",
    "sqlContext.sql(\"SELECT regexp_extract(text,'#([A-Za-z0-9_]+)') as handlers, count(*)\\\n",
    " as count FROM df WHERE text like '%#%' GROUP BY text\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of all of the people who are located in a particular city (e.g. Paris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the city's name to know how many user belongs to the city:Paris\n",
      "Paris\n",
      "+--------------------+--------------------+-----+\n",
      "|                text|                user|place|\n",
      "+--------------------+--------------------+-----+\n",
      "|'pivert', 'usager...|     Trendsmap Paris|Paris|\n",
      "|@jhornain Découvr...|            Codeclic|Paris|\n",
      "|Bon mardi ! OCTAV...|        EPHYRE Paris|Paris|\n",
      "|@YFEFRANCE Hey! Y...|  Helene PASQUALETTI|Paris|\n",
      "|日曜日の朝のパリ市庁舎前のスケート...|     Takaaki Kishida|Paris|\n",
      "|#Passing the #caf...|    Michael S. Osman|Paris|\n",
      "|I'm at ESP - Sall...|     Marie-Charlotte|Paris|\n",
      "|Grand Oral IT Nig...|    Juliette Laridan|Paris|\n",
      "|Etiopathe à Clama...| Boscheron Etiopathe|Paris|\n",
      "|エッフェル塔綺麗♪♪ @ Torr...|                 たいき|Paris|\n",
      "|Photos de @cinemi...|   Maxence d'Aubigny|Paris|\n",
      "|I'm at Avenue de ...|        Lionel Rigal|Paris|\n",
      "|Business meetings...|         Foued KEFIF|Paris|\n",
      "|#WakeUp #BelleJou...|                 Wil|Paris|\n",
      "|Paris: ☼ Temp: 3°...|         FranceBidet|Paris|\n",
      "|Fing, @la_fing es...|     Trendsmap Paris|Paris|\n",
      "|🗼🇫🇷 @ Level 2 ...|  مطر بن نومه الكتبي|Paris|\n",
      "|@trashdistance Je...|    Russell Williams|Paris|\n",
      "|I'm at Coutume In...|          Lauralou B|Paris|\n",
      "|Isabelle VIALLE, ...| ART aujourd'hui gal|Paris|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#i)\tGet a list of all of the people who are located in a particular city (e.g. Paris)\n",
    "city = raw_input(\"Please enter the city's name to know how many user belongs to the city:\")\n",
    "print city\n",
    "sqlContext.sql(\"SELECT text, user, place  FROM df WHERE place='{}'\".format(city)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Get country wise distribution of users, and find out which country ranks highest in terms of number of tweets, and number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n",
      "|             country|user|tweet|\n",
      "+--------------------+----+-----+\n",
      "|       United States|3595| 4841|\n",
      "|              France| 377|  737|\n",
      "|           Indonesia| 278|  370|\n",
      "|      United Kingdom| 301|  365|\n",
      "|              Brasil| 183|  256|\n",
      "|              Canada| 154|  172|\n",
      "|Republika ng Pili...| 127|  151|\n",
      "|           Argentina|  57|  104|\n",
      "|        South Africa|  73|   92|\n",
      "|           Australia|  77|   90|\n",
      "|               India|  56|   66|\n",
      "|              México|  47|   59|\n",
      "|                  日本|  52|   57|\n",
      "|              España|  32|   53|\n",
      "|            Malaysia|  37|   50|\n",
      "|               Kenya|   9|   44|\n",
      "|             Türkiye|  36|   42|\n",
      "|               Ghana|   9|   38|\n",
      "|            Colombia|  27|   33|\n",
      "|             Nigeria|  22|   33|\n",
      "+--------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT country, count(DISTINCT user) as user, count(text) as \\\n",
    "tweet FROM df GROUP BY country ORDER BY count(user) DESC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2K Find out number of tweets where a user is from France and mentions Paris in their tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|country|                id|               place|                text|                user|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "| France|572626703064952833|Allondrelle-la-Ma...|@Ashton5SOS be mo...|CrazyRockCountryLady|\n",
      "| France|572704775713628161|               Paris|'pivert', 'usager...|     Trendsmap Paris|\n",
      "| France|572685132475305984|               Paris|@jhornain Découvr...|            Codeclic|\n",
      "| France|572678334687150080|               Paris|Bon mardi ! OCTAV...|        EPHYRE Paris|\n",
      "| France|572664931973242880|              Chessy|Happy campers! #D...|       Candice LeRae|\n",
      "| France|572672815926796288|               Paris|@YFEFRANCE Hey! Y...|  Helene PASQUALETTI|\n",
      "| France|572648068832755712|         Saint-Avold|Opéra Garnier Par...|      Dominique Lang|\n",
      "| France|572557043942461440|              Chessy|#Disneyland #Pari...|      saif ali faraj|\n",
      "| France|572675772047101952|              Oissel|départ pour Paris 🚄|              Océane|\n",
      "| France|572601453728038914|               Paris|日曜日の朝のパリ市庁舎前のスケート...|     Takaaki Kishida|\n",
      "| France|572557144828088320|               Paris|#Passing the #caf...|    Michael S. Osman|\n",
      "| France|572693601253240832|               Paris|I'm at ESP - Sall...|     Marie-Charlotte|\n",
      "| France|572674665879093249|               Paris|Grand Oral IT Nig...|    Juliette Laridan|\n",
      "| France|572701867089633280|               Paris|Etiopathe à Clama...| Boscheron Etiopathe|\n",
      "| France|572551968251641856|               Paris|エッフェル塔綺麗♪♪ @ Torr...|                 たいき|\n",
      "| France|572702065979465728|               Paris|Photos de @cinemi...|   Maxence d'Aubigny|\n",
      "| France|572706435261472769|               Paris|I'm at Avenue de ...|        Lionel Rigal|\n",
      "| France|572700640612368384|               Paris|Business meetings...|         Foued KEFIF|\n",
      "| France|572695508013228032|               París|I'm at Jazz Club ...|                JLCV|\n",
      "| France|572661233763143680|               Paris|#WakeUp #BelleJou...|                 Wil|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT *  FROM df WHERE country='France' and text LIKE '%aris%'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Graph\n",
    "from pyspark.sql.functions import *\n",
    "from graphframes import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A Creating a GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trip = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load(\"file:///home/cloudera/Desktop/Assignment/trip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 src|                 dst|\n",
      "+--------------------+--------------------+\n",
      "|      Market at 10th|     2nd at Townsend|\n",
      "|Palo Alto Caltrai...|       Park at Olive|\n",
      "|    San Pedro Square|SJSU - San Salvad...|\n",
      "|   Market at Sansome|     Beale at Market|\n",
      "|       Market at 4th|      Market at 10th|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.registerTempTable(\"trip\")\n",
    "trip.cache()\n",
    "trip1 = trip.withColumnRenamed(\"Start Station\", \"src\").distinct()\n",
    "tripEdges = trip1.withColumnRenamed(\"End Station\", \"dst\").distinct()\n",
    "tripEdges = tripEdges.select(\"src\",\"dst\")\n",
    "tripEdges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true', inferschema='true').load(\"file:///home/cloudera/Desktop/Assignment/station.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  id|\n",
      "+--------------------+\n",
      "| San Salvador at 1st|\n",
      "|SJSU 4th at San C...|\n",
      "|Grant Avenue at C...|\n",
      "|California Ave Ca...|\n",
      "|Harry Bridges Pla...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station.registerTempTable(\"station\")\n",
    "station.cache()\n",
    "stationVertices = station.withColumnRenamed(\"name\", \"id\").distinct()\n",
    "stationVertices = stationVertices.select(\"id\")\n",
    "stationVertices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrame(v:[id: string], e:[src: string, dst: string])\n"
     ]
    }
   ],
   "source": [
    "g = GraphFrame(stationVertices, tripEdges)\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3BFind out number of incoming connections and outgoing connections for each node and print the top 10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|inDegree|\n",
      "+--------------------+--------+\n",
      "|San Francisco Cal...|   34810|\n",
      "|San Francisco Cal...|   22523|\n",
      "|Harry Bridges Pla...|   17810|\n",
      "|     2nd at Townsend|   15463|\n",
      "|     Townsend at 7th|   15422|\n",
      "|Embarcadero at Sa...|   15065|\n",
      "|   Market at Sansome|   13916|\n",
      "|   Steuart at Market|   13617|\n",
      "|Temporary Transba...|   12966|\n",
      "|  Powell Street BART|   10239|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b)\tFind out number of incoming connections and outgoing connections for each node \n",
    "#and print the top 10 nodes.\n",
    "ind = g.inDegrees.sort(desc(\"inDegree\"))\n",
    "ind.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                  id|outDegree|\n",
      "+--------------------+---------+\n",
      "|San Francisco Cal...|    26304|\n",
      "|San Francisco Cal...|    21758|\n",
      "|Harry Bridges Pla...|    17255|\n",
      "|Temporary Transba...|    14436|\n",
      "|Embarcadero at Sa...|    14158|\n",
      "|     2nd at Townsend|    14026|\n",
      "|     Townsend at 7th|    13752|\n",
      "|   Steuart at Market|    13687|\n",
      "|      Market at 10th|    11885|\n",
      "|   Market at Sansome|    11431|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# b)\tFind out number of incoming connections and outgoing connections for each node \n",
    "#and print the top 10 nodes.\n",
    "out = g.outDegrees.sort(desc(\"outDegree\"))\n",
    "out.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C Find out which are the most common direct routes that people take and print top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#c)\tFind out which are the most common direct routes that people take and print top 10.\n",
    "\n",
    "motifs = g.find(\"(a)-[directroute] -> (b)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                   a|                   b|count|\n",
      "+--------------------+--------------------+-----+\n",
      "|[San Francisco Ca...|   [Townsend at 7th]| 3748|\n",
      "|[Harry Bridges Pl...|[Embarcadero at S...| 3145|\n",
      "|   [2nd at Townsend]|[Harry Bridges Pl...| 2973|\n",
      "|   [Townsend at 7th]|[San Francisco Ca...| 2734|\n",
      "|[Harry Bridges Pl...|   [2nd at Townsend]| 2640|\n",
      "|[Embarcadero at F...|[San Francisco Ca...| 2439|\n",
      "| [Steuart at Market]|   [2nd at Townsend]| 2356|\n",
      "|[Embarcadero at S...| [Steuart at Market]| 2330|\n",
      "|   [Townsend at 7th]|[San Francisco Ca...| 2192|\n",
      "|[Temporary Transb...|[San Francisco Ca...| 2184|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "motifs.groupby(\"a\",\"b\").count().sort((desc(\"count\"))).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D From the analysis in b, see which are the stations where people most frequently start their trips but do not come back. (Hint: You might have to think of incoming connections as a ratio of outgoing connections). Print top 10 such stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+------------------+\n",
      "|                  id|inDegree|outDegree|         Start-End|\n",
      "+--------------------+--------+---------+------------------+\n",
      "|Grant Avenue at C...|    4319|     8337|1.9303079416531606|\n",
      "|       2nd at Folsom|    4727|     7999|1.6921937804104084|\n",
      "|Powell at Post (U...|    4134|     6425|  1.55418480890179|\n",
      "|          Mezes Park|     145|      212|1.4620689655172414|\n",
      "|Evelyn Park and Ride|     725|      978|1.3489655172413793|\n",
      "|     Beale at Market|    6330|     8359|1.3205371248025277|\n",
      "| Golden Gate at Polk|    2852|     3646|1.2784011220196354|\n",
      "|         Ryland Park|     880|     1120|1.2727272727272727|\n",
      "|San Francisco Cit...|    1627|     2052|1.2612169637369393|\n",
      "|Palo Alto Caltrai...|     900|     1116|              1.24|\n",
      "|  San Jose City Hall|     671|      832| 1.239940387481371|\n",
      "|South Van Ness at...|    4879|     5876|1.2043451526952245|\n",
      "|Redwood City Publ...|      98|      118|1.2040816326530612|\n",
      "|      Market at 10th|   10220|    11885|1.1629158512720157|\n",
      "|       St James Park|     729|      839|1.1508916323731138|\n",
      "|   2nd at South Park|    8253|     9469| 1.147340361080819|\n",
      "|    Adobe on Almaden|     490|      562| 1.146938775510204|\n",
      "|         MLK Library|     960|     1099|1.1447916666666667|\n",
      "|Rengstorff Avenue...|     441|      501|1.1360544217687074|\n",
      "|Redwood City Calt...|     799|      895|1.1201501877346682|\n",
      "+--------------------+--------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#d)\tFrom the analysis in b, see which are the stations where people most frequently \n",
    "#start their trips but do not come back. (Hint: You might \n",
    "#have to think of incoming connections as a ratio of outgoing connections). Print top 10 such stations.\n",
    "\n",
    "\n",
    "df_join = ind.join(out, ['id'])\n",
    "df_join = df_join.withColumn('Start-End', df_join['outDegree']/df_join['inDegree'])\n",
    "df_join.sort(desc('Start-End')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E Find all such patterns where any station a is connected to station b, b is connected to c, but c is not directly connected to a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edge = tripEdges.distinct()\n",
    "v = stationVertices.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_df = GraphFrame(v,edge)\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#e)\tFind all such patterns \n",
    "#where any station a is connected to station b, b is connected to c, but c is not directly connected to a.\n",
    "pattern1= graph_df.find(\"(a)-[] -> (b); (b)-[]->(c); !(c)-[]->(a)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   a|                   b|                   c|\n",
      "+--------------------+--------------------+--------------------+\n",
      "| [Franklin at Maple]|        [Mezes Park]|        [Mezes Park]|\n",
      "|[San Jose Diridon...|[San Jose Civic C...|[Mountain View Ca...|\n",
      "|[Rengstorff Avenu...|[Castro Street an...|     [Howard at 2nd]|\n",
      "|[San Francisco Ca...|[San Francisco Ca...|[San Antonio Calt...|\n",
      "|   [Townsend at 7th]|     [Market at 4th]|[Stanford in Redw...|\n",
      "|[Temporary Transb...|   [Beale at Market]|        [Mezes Park]|\n",
      "|[Grant Avenue at ...|   [Beale at Market]|        [Mezes Park]|\n",
      "|[Embarcadero at B...|[Powell Street BART]|[San Antonio Shop...|\n",
      "|[California Ave C...|[University and E...|[San Francisco Ca...|\n",
      "|     [Park at Olive]|[Palo Alto Caltra...|[Stanford in Redw...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## First 10 rows..\n",
    "pattern1.distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a PageRank algorithm to figure out which is the most important station in the entire graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                  id|           pagerank|\n",
      "+--------------------+-------------------+\n",
      "|San Jose Civic Ce...| 0.7850066002667163|\n",
      "|Redwood City Calt...| 2.0126980700870134|\n",
      "|San Mateo County ...| 0.7055535181356853|\n",
      "|Rengstorff Avenue...|0.44132804966440414|\n",
      "|South Van Ness at...|  0.552295911128788|\n",
      "|           Japantown| 0.8100868826750073|\n",
      "|    San Pedro Square|  1.222063710529912|\n",
      "|San Francisco Cit...|0.27914858766684636|\n",
      "|     Spear at Folsom|   0.55331286744544|\n",
      "|San Francisco Cal...| 1.9712236933192793|\n",
      "|Evelyn Park and Ride| 0.5361583702600942|\n",
      "|Paseo de San Antonio| 0.8866333561586521|\n",
      "|University and Em...| 1.1957276186540653|\n",
      "|Santa Clara Count...| 0.4570804856379124|\n",
      "|    Adobe on Almaden|0.47928670715376825|\n",
      "|  San Jose City Hall| 0.6157118631337893|\n",
      "|Embarcadero at Sa...| 1.3681133631508233|\n",
      "|San Antonio Caltr...| 0.9900409779366691|\n",
      "|   Market at Sansome| 1.2280531103488006|\n",
      "|       St James Park| 0.6591598371119667|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = g.pageRank(resetProbability=0.15, tol=0.01)\n",
    "results.vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                  id|          pagerank|\n",
      "+--------------------+------------------+\n",
      "|San Jose Diridon ...|3.5710727174436583|\n",
      "|San Francisco Cal...|2.9548304501454092|\n",
      "|Mountain View Cal...|2.2733423294327504|\n",
      "|Redwood City Calt...|2.0126980700870134|\n",
      "|San Francisco Cal...|1.9712236933192793|\n",
      "|Harry Bridges Pla...|1.6188585199762089|\n",
      "|     2nd at Townsend|1.4055254623119484|\n",
      "|Santa Clara at Al...| 1.393972040450786|\n",
      "|     Townsend at 7th|1.3893781153627542|\n",
      "|Embarcadero at Sa...|1.3681133631508233|\n",
      "|   Steuart at Market|1.3032272474235531|\n",
      "|Palo Alto Caltrai...|1.2639463072306332|\n",
      "|   Market at Sansome|1.2280531103488006|\n",
      "|    San Pedro Square| 1.222063710529912|\n",
      "|Temporary Transba...|1.2140210475942863|\n",
      "|University and Em...|1.1957276186540653|\n",
      "|Mountain View Cit...|1.0979929184245107|\n",
      "|San Antonio Shopp...|1.0251906345486324|\n",
      "|Stanford in Redwo...|1.0224956580157227|\n",
      "|San Antonio Caltr...|0.9900409779366691|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top Station\n",
    "results.vertices.sort(desc(\"pagerank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
